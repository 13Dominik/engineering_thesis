{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:01:57.160170676Z",
     "start_time": "2023-05-07T11:01:57.113678589Z"
    }
   },
   "outputs": [],
   "source": [
    "import  nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dominik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/dominik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dominik/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/dominik/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "['إذ',\n 'إذا',\n 'إذما',\n 'إذن',\n 'أف',\n 'أقل',\n 'أكثر',\n 'ألا',\n 'إلا',\n 'التي',\n 'الذي',\n 'الذين',\n 'اللاتي',\n 'اللائي',\n 'اللتان',\n 'اللتيا',\n 'اللتين',\n 'اللذان',\n 'اللذين',\n 'اللواتي',\n 'إلى',\n 'إليك',\n 'إليكم',\n 'إليكما',\n 'إليكن',\n 'أم',\n 'أما',\n 'أما',\n 'إما',\n 'أن',\n 'إن',\n 'إنا',\n 'أنا',\n 'أنت',\n 'أنتم',\n 'أنتما',\n 'أنتن',\n 'إنما',\n 'إنه',\n 'أنى',\n 'أنى',\n 'آه',\n 'آها',\n 'أو',\n 'أولاء',\n 'أولئك',\n 'أوه',\n 'آي',\n 'أي',\n 'أيها',\n 'إي',\n 'أين',\n 'أين',\n 'أينما',\n 'إيه',\n 'بخ',\n 'بس',\n 'بعد',\n 'بعض',\n 'بك',\n 'بكم',\n 'بكم',\n 'بكما',\n 'بكن',\n 'بل',\n 'بلى',\n 'بما',\n 'بماذا',\n 'بمن',\n 'بنا',\n 'به',\n 'بها',\n 'بهم',\n 'بهما',\n 'بهن',\n 'بي',\n 'بين',\n 'بيد',\n 'تلك',\n 'تلكم',\n 'تلكما',\n 'ته',\n 'تي',\n 'تين',\n 'تينك',\n 'ثم',\n 'ثمة',\n 'حاشا',\n 'حبذا',\n 'حتى',\n 'حيث',\n 'حيثما',\n 'حين',\n 'خلا',\n 'دون',\n 'ذا',\n 'ذات',\n 'ذاك',\n 'ذان',\n 'ذانك',\n 'ذلك',\n 'ذلكم',\n 'ذلكما',\n 'ذلكن',\n 'ذه',\n 'ذو',\n 'ذوا',\n 'ذواتا',\n 'ذواتي',\n 'ذي',\n 'ذين',\n 'ذينك',\n 'ريث',\n 'سوف',\n 'سوى',\n 'شتان',\n 'عدا',\n 'عسى',\n 'عل',\n 'على',\n 'عليك',\n 'عليه',\n 'عما',\n 'عن',\n 'عند',\n 'غير',\n 'فإذا',\n 'فإن',\n 'فلا',\n 'فمن',\n 'في',\n 'فيم',\n 'فيما',\n 'فيه',\n 'فيها',\n 'قد',\n 'كأن',\n 'كأنما',\n 'كأي',\n 'كأين',\n 'كذا',\n 'كذلك',\n 'كل',\n 'كلا',\n 'كلاهما',\n 'كلتا',\n 'كلما',\n 'كليكما',\n 'كليهما',\n 'كم',\n 'كم',\n 'كما',\n 'كي',\n 'كيت',\n 'كيف',\n 'كيفما',\n 'لا',\n 'لاسيما',\n 'لدى',\n 'لست',\n 'لستم',\n 'لستما',\n 'لستن',\n 'لسن',\n 'لسنا',\n 'لعل',\n 'لك',\n 'لكم',\n 'لكما',\n 'لكن',\n 'لكنما',\n 'لكي',\n 'لكيلا',\n 'لم',\n 'لما',\n 'لن',\n 'لنا',\n 'له',\n 'لها',\n 'لهم',\n 'لهما',\n 'لهن',\n 'لو',\n 'لولا',\n 'لوما',\n 'لي',\n 'لئن',\n 'ليت',\n 'ليس',\n 'ليسا',\n 'ليست',\n 'ليستا',\n 'ليسوا',\n 'ما',\n 'ماذا',\n 'متى',\n 'مذ',\n 'مع',\n 'مما',\n 'ممن',\n 'من',\n 'منه',\n 'منها',\n 'منذ',\n 'مه',\n 'مهما',\n 'نحن',\n 'نحو',\n 'نعم',\n 'ها',\n 'هاتان',\n 'هاته',\n 'هاتي',\n 'هاتين',\n 'هاك',\n 'هاهنا',\n 'هذا',\n 'هذان',\n 'هذه',\n 'هذي',\n 'هذين',\n 'هكذا',\n 'هل',\n 'هلا',\n 'هم',\n 'هما',\n 'هن',\n 'هنا',\n 'هناك',\n 'هنالك',\n 'هو',\n 'هؤلاء',\n 'هي',\n 'هيا',\n 'هيت',\n 'هيهات',\n 'والذي',\n 'والذين',\n 'وإذ',\n 'وإذا',\n 'وإن',\n 'ولا',\n 'ولكن',\n 'ولو',\n 'وما',\n 'ومن',\n 'وهو',\n 'يا',\n 'أبٌ',\n 'أخٌ',\n 'حمٌ',\n 'فو',\n 'أنتِ',\n 'يناير',\n 'فبراير',\n 'مارس',\n 'أبريل',\n 'مايو',\n 'يونيو',\n 'يوليو',\n 'أغسطس',\n 'سبتمبر',\n 'أكتوبر',\n 'نوفمبر',\n 'ديسمبر',\n 'جانفي',\n 'فيفري',\n 'مارس',\n 'أفريل',\n 'ماي',\n 'جوان',\n 'جويلية',\n 'أوت',\n 'كانون',\n 'شباط',\n 'آذار',\n 'نيسان',\n 'أيار',\n 'حزيران',\n 'تموز',\n 'آب',\n 'أيلول',\n 'تشرين',\n 'دولار',\n 'دينار',\n 'ريال',\n 'درهم',\n 'ليرة',\n 'جنيه',\n 'قرش',\n 'مليم',\n 'فلس',\n 'هللة',\n 'سنتيم',\n 'يورو',\n 'ين',\n 'يوان',\n 'شيكل',\n 'واحد',\n 'اثنان',\n 'ثلاثة',\n 'أربعة',\n 'خمسة',\n 'ستة',\n 'سبعة',\n 'ثمانية',\n 'تسعة',\n 'عشرة',\n 'أحد',\n 'اثنا',\n 'اثني',\n 'إحدى',\n 'ثلاث',\n 'أربع',\n 'خمس',\n 'ست',\n 'سبع',\n 'ثماني',\n 'تسع',\n 'عشر',\n 'ثمان',\n 'سبت',\n 'أحد',\n 'اثنين',\n 'ثلاثاء',\n 'أربعاء',\n 'خميس',\n 'جمعة',\n 'أول',\n 'ثان',\n 'ثاني',\n 'ثالث',\n 'رابع',\n 'خامس',\n 'سادس',\n 'سابع',\n 'ثامن',\n 'تاسع',\n 'عاشر',\n 'حادي',\n 'أ',\n 'ب',\n 'ت',\n 'ث',\n 'ج',\n 'ح',\n 'خ',\n 'د',\n 'ذ',\n 'ر',\n 'ز',\n 'س',\n 'ش',\n 'ص',\n 'ض',\n 'ط',\n 'ظ',\n 'ع',\n 'غ',\n 'ف',\n 'ق',\n 'ك',\n 'ل',\n 'م',\n 'ن',\n 'ه',\n 'و',\n 'ي',\n 'ء',\n 'ى',\n 'آ',\n 'ؤ',\n 'ئ',\n 'أ',\n 'ة',\n 'ألف',\n 'باء',\n 'تاء',\n 'ثاء',\n 'جيم',\n 'حاء',\n 'خاء',\n 'دال',\n 'ذال',\n 'راء',\n 'زاي',\n 'سين',\n 'شين',\n 'صاد',\n 'ضاد',\n 'طاء',\n 'ظاء',\n 'عين',\n 'غين',\n 'فاء',\n 'قاف',\n 'كاف',\n 'لام',\n 'ميم',\n 'نون',\n 'هاء',\n 'واو',\n 'ياء',\n 'همزة',\n 'ي',\n 'نا',\n 'ك',\n 'كن',\n 'ه',\n 'إياه',\n 'إياها',\n 'إياهما',\n 'إياهم',\n 'إياهن',\n 'إياك',\n 'إياكما',\n 'إياكم',\n 'إياك',\n 'إياكن',\n 'إياي',\n 'إيانا',\n 'أولالك',\n 'تانِ',\n 'تانِك',\n 'تِه',\n 'تِي',\n 'تَيْنِ',\n 'ثمّ',\n 'ثمّة',\n 'ذانِ',\n 'ذِه',\n 'ذِي',\n 'ذَيْنِ',\n 'هَؤلاء',\n 'هَاتانِ',\n 'هَاتِه',\n 'هَاتِي',\n 'هَاتَيْنِ',\n 'هَذا',\n 'هَذانِ',\n 'هَذِه',\n 'هَذِي',\n 'هَذَيْنِ',\n 'الألى',\n 'الألاء',\n 'أل',\n 'أنّى',\n 'أيّ',\n 'ّأيّان',\n 'أنّى',\n 'أيّ',\n 'ّأيّان',\n 'ذيت',\n 'كأيّ',\n 'كأيّن',\n 'بضع',\n 'فلان',\n 'وا',\n 'آمينَ',\n 'آهِ',\n 'آهٍ',\n 'آهاً',\n 'أُفٍّ',\n 'أُفٍّ',\n 'أفٍّ',\n 'أمامك',\n 'أمامكَ',\n 'أوّهْ',\n 'إلَيْكَ',\n 'إلَيْكَ',\n 'إليكَ',\n 'إليكنّ',\n 'إيهٍ',\n 'بخٍ',\n 'بسّ',\n 'بَسْ',\n 'بطآن',\n 'بَلْهَ',\n 'حاي',\n 'حَذارِ',\n 'حيَّ',\n 'حيَّ',\n 'دونك',\n 'رويدك',\n 'سرعان',\n 'شتانَ',\n 'شَتَّانَ',\n 'صهْ',\n 'صهٍ',\n 'طاق',\n 'طَق',\n 'عَدَسْ',\n 'كِخ',\n 'مكانَك',\n 'مكانَك',\n 'مكانَك',\n 'مكانكم',\n 'مكانكما',\n 'مكانكنّ',\n 'نَخْ',\n 'هاكَ',\n 'هَجْ',\n 'هلم',\n 'هيّا',\n 'هَيْهات',\n 'وا',\n 'واهاً',\n 'وراءَك',\n 'وُشْكَانَ',\n 'وَيْ',\n 'يفعلان',\n 'تفعلان',\n 'يفعلون',\n 'تفعلون',\n 'تفعلين',\n 'اتخذ',\n 'ألفى',\n 'تخذ',\n 'ترك',\n 'تعلَّم',\n 'جعل',\n 'حجا',\n 'حبيب',\n 'خال',\n 'حسب',\n 'خال',\n 'درى',\n 'رأى',\n 'زعم',\n 'صبر',\n 'ظنَّ',\n 'عدَّ',\n 'علم',\n 'غادر',\n 'ذهب',\n 'وجد',\n 'ورد',\n 'وهب',\n 'أسكن',\n 'أطعم',\n 'أعطى',\n 'رزق',\n 'زود',\n 'سقى',\n 'كسا',\n 'أخبر',\n 'أرى',\n 'أعلم',\n 'أنبأ',\n 'حدَث',\n 'خبَّر',\n 'نبَّا',\n 'أفعل به',\n 'ما أفعله',\n 'بئس',\n 'ساء',\n 'طالما',\n 'قلما',\n 'لات',\n 'لكنَّ',\n 'ءَ',\n 'أجل',\n 'إذاً',\n 'أمّا',\n 'إمّا',\n 'إنَّ',\n 'أنًّ',\n 'أى',\n 'إى',\n 'أيا',\n 'ب',\n 'ثمَّ',\n 'جلل',\n 'جير',\n 'رُبَّ',\n 'س',\n 'علًّ',\n 'ف',\n 'كأنّ',\n 'كلَّا',\n 'كى',\n 'ل',\n 'لات',\n 'لعلَّ',\n 'لكنَّ',\n 'لكنَّ',\n 'م',\n 'نَّ',\n 'هلّا',\n 'وا',\n 'أل',\n 'إلّا',\n 'ت',\n 'ك',\n 'لمّا',\n 'ن',\n 'ه',\n 'و',\n 'ا',\n 'ي',\n 'تجاه',\n 'تلقاء',\n 'جميع',\n 'حسب',\n 'سبحان',\n 'شبه',\n 'لعمر',\n 'مثل',\n 'معاذ',\n 'أبو',\n 'أخو',\n 'حمو',\n 'فو',\n 'مئة',\n 'مئتان',\n 'ثلاثمئة',\n 'أربعمئة',\n 'خمسمئة',\n 'ستمئة',\n 'سبعمئة',\n 'ثمنمئة',\n 'تسعمئة',\n 'مائة',\n 'ثلاثمائة',\n 'أربعمائة',\n 'خمسمائة',\n 'ستمائة',\n 'سبعمائة',\n 'ثمانمئة',\n 'تسعمائة',\n 'عشرون',\n 'ثلاثون',\n 'اربعون',\n 'خمسون',\n 'ستون',\n 'سبعون',\n 'ثمانون',\n 'تسعون',\n 'عشرين',\n 'ثلاثين',\n 'اربعين',\n 'خمسين',\n 'ستين',\n 'سبعين',\n 'ثمانين',\n 'تسعين',\n 'بضع',\n 'نيف',\n 'أجمع',\n 'جميع',\n 'عامة',\n 'عين',\n 'نفس',\n 'لا سيما',\n 'أصلا',\n 'أهلا',\n 'أيضا',\n 'بؤسا',\n 'بعدا',\n 'بغتة',\n 'تعسا',\n 'حقا',\n 'حمدا',\n 'خلافا',\n 'خاصة',\n 'دواليك',\n 'سحقا',\n 'سرا',\n 'سمعا',\n 'صبرا',\n 'صدقا',\n 'صراحة',\n 'طرا',\n 'عجبا',\n 'عيانا',\n 'غالبا',\n 'فرادى',\n 'فضلا',\n 'قاطبة',\n 'كثيرا',\n 'لبيك',\n 'معاذ',\n 'أبدا',\n 'إزاء',\n 'أصلا',\n 'الآن',\n 'أمد',\n 'أمس',\n 'آنفا',\n 'آناء',\n 'أنّى',\n 'أول',\n 'أيّان',\n 'تارة',\n 'ثمّ',\n 'ثمّة',\n 'حقا',\n 'صباح',\n 'مساء',\n 'ضحوة',\n 'عوض',\n 'غدا',\n 'غداة',\n 'قطّ',\n 'كلّما',\n 'لدن',\n 'لمّا',\n 'مرّة',\n 'قبل',\n 'خلف',\n 'أمام',\n 'فوق',\n 'تحت',\n 'يمين',\n 'شمال',\n 'ارتدّ',\n 'استحال',\n 'أصبح',\n 'أضحى',\n 'آض',\n 'أمسى',\n 'انقلب',\n 'بات',\n 'تبدّل',\n 'تحوّل',\n 'حار',\n 'رجع',\n 'راح',\n 'صار',\n 'ظلّ',\n 'عاد',\n 'غدا',\n 'كان',\n 'ما انفك',\n 'ما برح',\n 'مادام',\n 'مازال',\n 'مافتئ',\n 'ابتدأ',\n 'أخذ',\n 'اخلولق',\n 'أقبل',\n 'انبرى',\n 'أنشأ',\n 'أوشك',\n 'جعل',\n 'حرى',\n 'شرع',\n 'طفق',\n 'علق',\n 'قام',\n 'كرب',\n 'كاد',\n 'هبّ']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('arabic')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:01:57.319447114Z",
     "start_time": "2023-05-07T11:01:57.261893757Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "  if treebank_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif treebank_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif treebank_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif treebank_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return wordnet.NOUN\n",
    "\n",
    "class LemmaTokenizer:\n",
    "  def __init__(self):\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "  def __call__(self, doc):\n",
    "    tokens = word_tokenize(doc)\n",
    "    words_and_tags = nltk.pos_tag(tokens)\n",
    "    return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) \\\n",
    "            for word, tag in words_and_tags]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:02:59.381223313Z",
     "start_time": "2023-05-07T11:02:59.322052640Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'like', 'me', '.']\n",
      "['She', 'be', 'walk', 'with', 'me', 'She', 'walk', 'bitch']\n"
     ]
    }
   ],
   "source": [
    "s = [\"She likes me.\", 'She was walking with me She walked bitch']\n",
    "l = LemmaTokenizer()\n",
    "print(l(s[0]))\n",
    "print(l(s[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:03:59.323085775Z",
     "start_time": "2023-05-07T11:03:59.276024591Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominik/anaconda3/envs/dominiktomalczyk/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "['.', 'be', 'bitch', 'like', 'me', 'she', 'walk', 'with']"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "Xtrain = vectorizer.fit_transform(s)\n",
    "Xtrain.toarray()\n",
    "vectorizer.get_feature_names()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:04:09.388772709Z",
     "start_time": "2023-05-07T11:04:09.376937176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "['She', 'like', 'me', '.']"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = LemmaTokenizer()\n",
    "c(s[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:05:20.966516105Z",
     "start_time": "2023-05-07T11:05:20.961263969Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 2, 3, 4, 3, 0, 5, 6, 1], dtype=int32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:01.530083525Z",
     "start_time": "2023-05-07T10:57:01.523664460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 1, 1, 1, 0, 0],\n       [1, 1, 0, 1, 2, 2, 1]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:02.024941472Z",
     "start_time": "2023-05-07T10:57:02.013183939Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "sentence = \"The following is stupid\".split()\n",
    "sentence2 = \"cat was following the dog\".split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:03.769767203Z",
     "start_time": "2023-05-07T10:57:03.765023951Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('following', 'NN'), ('is', 'VBZ'), ('stupid', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(sentence))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:04.390237572Z",
     "start_time": "2023-05-07T10:57:04.386640243Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'NN'), ('was', 'VBD'), ('following', 'VBG'), ('the', 'DT'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(sentence2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:05.449317565Z",
     "start_time": "2023-05-07T10:57:05.444350350Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 1, 1, 1, 0, 0],\n       [1, 1, 0, 1, 2, 2, 1]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "Xtrain = vectorizer.fit_transform(s)\n",
    "Xtrain.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:05.933437410Z",
     "start_time": "2023-05-07T10:57:05.926749329Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 1, 1, 0, 0],\n       [1, 1, 0, 1, 2, 1]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=[\"she\"])\n",
    "Xtrain = vectorizer.fit_transform(s)\n",
    "Xtrain.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:06.627838855Z",
     "start_time": "2023-05-07T10:57:06.623375134Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:08.104245815Z",
     "start_time": "2023-05-07T10:57:07.716011297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "v = np.array([1,2,3])\n",
    "v2 = v/np.linalg.norm(v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:09.147648980Z",
     "start_time": "2023-05-07T10:57:09.143156562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "3.7416573867739413"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T10:57:10.042924671Z",
     "start_time": "2023-05-07T10:57:10.036668125Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "v1 = np.array([1,2])\n",
    "v2 = np.array([3,7])\n",
    "v3 = np.array([7,8])\n",
    "\n",
    "v1 = normalize(v1.reshape(1, -1))\n",
    "v2 = normalize(v2.reshape(1, -1))\n",
    "v3 = normalize(v3.reshape(1, -1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T10:45:30.559944967Z",
     "start_time": "2023-05-06T10:45:30.482955735Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00172563]]\n",
      "[[0.05874737]]\n",
      "[[0.03238273]]\n",
      "[[0.25449058]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "print(cosine_distances(v1.reshape(1,-1),v2.reshape(1, -1)))\n",
    "print((euclidean_distances(v1.reshape(1,-1),v2.reshape(1, -1))))\n",
    "\n",
    "print(cosine_distances(v1.reshape(1,-1),v3.reshape(1, -1)))\n",
    "print(euclidean_distances(v1.reshape(1,-1),v3.reshape(1, -1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T10:45:30.753494725Z",
     "start_time": "2023-05-06T10:45:30.736647121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "18.765743525552985"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.03238273 / 0.00172563"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T10:45:31.321086749Z",
     "start_time": "2023-05-06T10:45:31.315180114Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "18.76577760749242"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.25449058 / 0.05874737)**2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T10:45:31.757195161Z",
     "start_time": "2023-05-06T10:45:31.747860616Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.26726124, 0.53452248, 0.80178373]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1,2,3])\n",
    "\n",
    "v1 = normalize(v1.reshape(1, -1))\n",
    "v1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T10:45:33.520003533Z",
     "start_time": "2023-05-06T10:45:33.504998233Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(v1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T10:45:33.832545452Z",
     "start_time": "2023-05-06T10:45:33.826864410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.43855558, 0.        , 0.        , 0.35854792, 0.35854792,\n        0.54170339, 0.        , 0.        , 0.35854792, 0.        ,\n        0.35854792],\n       [0.35064154, 0.        , 0.        , 0.28667243, 0.57334487,\n        0.        , 0.        , 0.54934773, 0.28667243, 0.        ,\n        0.28667243],\n       [0.30100231, 0.        , 0.47157828, 0.24608911, 0.24608911,\n        0.        , 0.47157828, 0.        , 0.24608911, 0.47157828,\n        0.24608911],\n       [0.        , 0.60735961, 0.        , 0.31694544, 0.31694544,\n        0.4788493 , 0.        , 0.        , 0.31694544, 0.        ,\n        0.31694544]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "   'And this is the documents third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "X.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:01:01.086692779Z",
     "start_time": "2023-05-07T11:01:01.006174162Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['.', '?', 'and', 'be', 'document', 'first', 'one', 'second', 'the',\n       'third', 'this'], dtype=object)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:01:03.288393638Z",
     "start_time": "2023-05-07T11:01:03.284408601Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.46979139, 0.        , 0.58028582, 0.38408524,\n        0.        , 0.        , 0.38408524, 0.        , 0.38408524],\n       [0.        , 0.6876236 , 0.        , 0.        , 0.28108867,\n        0.        , 0.53864762, 0.28108867, 0.        , 0.28108867],\n       [0.45563143, 0.        , 0.45563143, 0.        , 0.23776738,\n        0.45563143, 0.        , 0.23776738, 0.45563143, 0.23776738],\n       [0.        , 0.46979139, 0.        , 0.58028582, 0.38408524,\n        0.        , 0.        , 0.38408524, 0.        , 0.38408524]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "X.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:01:05.932539041Z",
     "start_time": "2023-05-07T11:01:05.894649197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['and', 'document', 'documents', 'first', 'is', 'one', 'second',\n       'the', 'third', 'this'], dtype=object)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-07T11:01:08.361596740Z",
     "start_time": "2023-05-07T11:01:08.300337255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n        0.53864762, 0.28108867, 0.        , 0.28108867]])"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T19:13:21.198840830Z",
     "start_time": "2023-05-05T19:13:21.190572993Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t4\n",
      "  (0, 1)\t1\n",
      "  (1, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "y = ['apple black cook black black black', 'cook delay eagle', 'delay eagle fuck']\n",
    "vectorizer = CountVectorizer(max_features=2)\n",
    "ou = vectorizer.fit_transform(y)\n",
    "print(ou)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T20:13:19.080926219Z",
     "start_time": "2023-05-05T20:13:19.067202419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[4, 1],\n       [0, 1],\n       [0, 0]])"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T20:13:19.237771214Z",
     "start_time": "2023-05-05T20:13:19.199371644Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dominik/anaconda3/envs/dominiktomalczyk/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "['black', 'cook']"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T20:13:19.386532343Z",
     "start_time": "2023-05-05T20:13:19.343874277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T20:13:19.531775581Z",
     "start_time": "2023-05-05T20:13:19.492680377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-05 22:35:29--  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\r\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\r\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 5085081 (4.8M) [text/csv]\r\n",
      "Saving to: ‘bbc_text_cls.csv’\r\n",
      "\r\n",
      "bbc_text_cls.csv    100%[===================>]   4.85M  1.71MB/s    in 2.8s    \r\n",
      "\r\n",
      "2023-05-05 22:35:33 (1.71 MB/s) - ‘bbc_text_cls.csv’ saved [5085081/5085081]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T20:35:34.050476461Z",
     "start_time": "2023-05-05T20:35:28.998717684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dominik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv('bbc_text_cls.csv')\n",
    "# populate word2idx\n",
    "# convert documents into sequences of ints / ids / indices\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "tokenized_docs = []\n",
    "for doc in df['text']:\n",
    "  words = word_tokenize(doc.lower())\n",
    "  doc_as_int = []\n",
    "  for word in words:\n",
    "    if word not in word2idx:\n",
    "      word2idx[word] = idx\n",
    "      idx += 1\n",
    "\n",
    "    # save for later\n",
    "    doc_as_int.append(word2idx[word])\n",
    "  tokenized_docs.append(doc_as_int)\n",
    "# reverse mapping\n",
    "# if you do it smarter you can store it as a list\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "# number of documents\n",
    "N = len(df['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:20.983140443Z",
     "start_time": "2023-05-06T11:41:15.897334813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "2225"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:20.988609459Z",
     "start_time": "2023-05-06T11:41:20.984731371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "34762"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of words\n",
    "V = len(word2idx)\n",
    "V"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:20.992746181Z",
     "start_time": "2023-05-06T11:41:20.989070917Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 4., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 1., 1., 1.]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate term-frequency matrix\n",
    "# note: could have also used count vectorizer\n",
    "tf = np.zeros((N, V))\n",
    "# populate term-frequency counts\n",
    "for i, doc_as_int in enumerate(tokenized_docs):\n",
    "  for j in doc_as_int:\n",
    "    tf[i, j] += 1\n",
    "\n",
    "tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:21.401526599Z",
     "start_time": "2023-05-06T11:41:20.994784104Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ True,  True,  True, ..., False, False, False],\n       [False, False,  True, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ...,  True,  True,  True]])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf > 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:21.520649042Z",
     "start_time": "2023-05-06T11:41:21.403470992Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 12, 204, 127, ...,   1,   1,   1])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compute IDF\n",
    "document_freq = np.sum(tf > 0, axis=0)\n",
    "document_freq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:21.602566Z",
     "start_time": "2023-05-06T11:41:21.521868854Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([5.22260554, 2.3893922 , 2.86332511, ..., 7.70751219, 7.70751219,\n       7.70751219])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# document frequency (shape = (V,))\n",
    "idf = np.log(N / document_freq)\n",
    "idf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:21.609114725Z",
     "start_time": "2023-05-06T11:41:21.604245532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.22260554, 9.5575688 , 2.86332511, ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 2.86332511, ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 7.70751219, 7.70751219,\n        7.70751219]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute TF-IDF\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:21.898040220Z",
     "start_time": "2023-05-06T11:41:21.652100632Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "1346"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "# pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
    "i = np.random.choice(N)\n",
    "i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:21.903005336Z",
     "start_time": "2023-05-06T11:41:21.900132096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: business\n",
      "Text: China keeps tight rein on credit\n",
      "Top 5 terms:\n",
      "china\n",
      "breakneck\n",
      "credit\n",
      "%\n",
      "rapid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 15\n",
    "\n",
    "row = df.iloc[i]\n",
    "print(\"Label:\", row['labels'])\n",
    "print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
    "print(\"Top 5 terms:\")\n",
    "\n",
    "scores = tf_idf[i]\n",
    "indices = (-scores).argsort()\n",
    "\n",
    "for j in indices[:5]:\n",
    "  print(idx2word[j])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:43:30.078049190Z",
     "start_time": "2023-05-06T11:43:30.073786678Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[i]\n",
    "for elem in ['China', 'breakneck', 'credit', '%', 'rapid']:\n",
    "    print(elem in row['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:43:46.684462838Z",
     "start_time": "2023-05-06T11:43:46.677335996Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "'Athens memories soar above lows\\n\\nWell, it\\'s goodbye to another Olympic year and as usual there were plenty of highs and lows in Athens.\\n\\nObviously, there\\'s no getting away from the differing fortunes of Kelly Holmes and Paula Radcliffe. But I want to remind you of a few more events that made 2004 another year to remember - or forget - for athletics. \\n\\n \\n\\nOne of my favourite Olympic moments was Kelly\\'s success in the 800m.\\n\\nWinning that race was the key to her success because if she won that then the 1500m would be a bit of a formality. Kelly had been full of \"should I, shouldn\\'t I?\" thoughts about going for the double in Athens. I thought why wouldn\\'t you do the 800m, it\\'s your best event? It was such good fun to commentate on her 1500m and it was nice to be able to be part of her Athens story. \\n\\n \\n\\nThe victory for the British men\\'s 4x100m relay team was a bit of a surprise but a great climax to the Games. I think the four of them - Jason Gardener, Darren Campbell, Marlon Devonish and Mark Lewis-Francis - knew deep down that it was their best chance of a medal. The lads had run poorly in the individual sprints so maybe they did lift their game when they knew something was really at stake. \\n\\n \\n\\nHicham El Guerrouj\\'s Olympic double is a much bigger achievement than Kelly\\'s on a global scale.\\n\\nHe was the first man since for 80 years to win both the 1500m and 5,000m titles. As soon as he had added the 5,000m crown and I had finished commentating, I jumped up, ran down the stairs, pushed everyone out the way and just gave him a big hug. He is one of the few African runners who has embraced the tradition of the mile and he loves to hear all the Roger Bannister stories. Hicham is someone I enjoy having a bit of time with, even though my French and his English are not very good. \\n\\n \\n\\nWhat happened to Paula in Athens this year is the obvious low on a personal level and for the expectations of the nation as well. There were a set of circumstances around Athens that conspired to produce a very dramatic ending which I think has been greatly misunderstood. Dropping out of the marathon was the right thing to do but starting in the 10,000m five days later was not wise. That was her heart and not her head reacting. Paula had a lot of little things going wrong in her preparation and on the day.\\n\\nThings like niggling injuries, not being able to do all her running sessions and feeling the pressure of the race looming ahead of her. I think she came to the start line in Athens physically and emotionally drained. And if even the smallest thing doesn\\'t feel right when you are preparing to race a marathon, 10 miles down the road it will hit you like a brick wall. The positive thing to take from Paula\\'s Olympics it that she will have learned a lot from it and so will a lot of people - including me. \\n\\n \\n\\nPurely as a race, Paula\\'s victory in the New York Marathon has to go down as one of the most thrilling. It was so nip-and-tuck between her and Kenya\\'s Susan Chepkemei and you don\\'t usually get that kind of excitement in marathons. It was also a real delight for all athletics fans because, to use one of my favourite words, Paula showed real \"bouncebackability\". And it was a bit of a rarity for me too because I genuinely did not have an inkling how the race was going to pan out. \\n\\n \\n\\nKelly and the 4x100m boys\\' victories papered over the cracks in the general performance of the British team. We should be concerned that we\\'re not producing enough people who are capable of reaching finals at senior level.\\n\\nThe only individual men\\'s finalist on the track was Michael East in the 1500m. I am beginning to look down and wonder where are the new breed? And that\\'s where things begin to look even gloomier for British athletics as we did not win any medals at the world junior championships in Italy. Dani Barnes came fourth in the 1500m and she was the highest finisher for Team GB. The thing is if we don\\'t have athletes getting into the finals at junior level then it really doesn\\'t look good for the Beijing Olympics and beyond. \\n\\n \\n\\nI tell you what I really enjoyed this year, Benita Johnson winning the world cross country championships back in March. In the absence of Paula, we tend to think of the event as something of an African preserve. So to have an Australian come up and deliver such a surprise was something special. \\n\\n \\n\\nTo be honest, I\\'m getting bored with all the drug scandals, especially Balco. I just wish the whole thing would come to a head so we can move on.\\n\\nHaving said that, I\\'m always pleased when drugs cheats are caught because it shows the sport is standing up to it and not turning a blind eye anymore. And one of the positive things to come out of Balco is people are starting to blow the whistle. We need more people to come forward and help the authorities kick out the cheats. As regards the case against Greek sprinters Kostas Kenteris and Katerina Thanou, well suspicions have been hanging over Kenteris for a while. The bottom line is we cannot keep letting drugs damage the sport because if we do then it stops everyone enjoying it.'"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T11:41:50.169244129Z",
     "start_time": "2023-05-06T11:41:50.153351669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [67], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T12:48:59.209575250Z",
     "start_time": "2023-05-06T12:48:59.190876244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# przygotowanie zbioru tekstów\n",
    "sentences = [[\"this\", \"is\", \"the\", \"first\", \"sentence\", \"for\", \"word2vec\"],\n",
    "             [\"this\", \"is\", \"the\", \"second\", \"sentence\"],\n",
    "             [\"yet\", \"another\", \"sentence\"],\n",
    "             [\"one\", \"more\", \"sentence\"],\n",
    "             [\"and\", \"the\", \"final\", \"sentence\"]]\n",
    "\n",
    "# trenowanie modelu Word2Vec\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# wyświetlenie wektora reprezentującego słowo \"sentence\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T12:51:53.556443175Z",
     "start_time": "2023-05-06T12:51:53.543779369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': 0,\n 'the': 1,\n 'is': 2,\n 'this': 3,\n 'final': 4,\n 'and': 5,\n 'more': 6,\n 'one': 7,\n 'another': 8,\n 'yet': 9,\n 'second': 10,\n 'word2vec': 11,\n 'for': 12,\n 'first': 13}"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.key_to_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T12:51:53.710279667Z",
     "start_time": "2023-05-06T12:51:53.701991116Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'dupa' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [81], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model\u001B[38;5;241m.\u001B[39mwv\u001B[38;5;241m.\u001B[39mget_vector(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdupa\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/dominiktomalczyk/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001B[0m, in \u001B[0;36mKeyedVectors.get_vector\u001B[0;34m(self, key, norm)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_vector\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    423\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001B[39;00m\n\u001B[1;32m    424\u001B[0m \n\u001B[1;32m    425\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    444\u001B[0m \n\u001B[1;32m    445\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 446\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m norm:\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfill_norms()\n",
      "File \u001B[0;32m~/anaconda3/envs/dominiktomalczyk/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001B[0m, in \u001B[0;36mKeyedVectors.get_index\u001B[0;34m(self, key, default)\u001B[0m\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default\n\u001B[1;32m    419\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 420\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKey \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not present\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Key 'dupa' not present\""
     ]
    }
   ],
   "source": [
    "model.wv.get_vector('dupa')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T12:59:21.131533331Z",
     "start_time": "2023-05-06T12:59:21.078838377Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T12:51:36.098572142Z",
     "start_time": "2023-05-06T12:51:36.084221863Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

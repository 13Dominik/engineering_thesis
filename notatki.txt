Count Vector - po prostu zlicza słowa (CountVectorizer z sklearn)
CountVectorizer oblicza częstość występowania każdego słowa w każdym
dokumencie korpusu i zlicza łączną liczbę wystąpień każdego słowa we wszystkich dokumentach.
Następnie, słowa są sortowane malejąco według liczby wystąpień, a max_features najczęściej
występujących słów są wybierane do utworzenia słownika. (TYM SOBIE PORADZE Z BRAKUJĄCYMI SŁOWAMI)


Tokenization - zazwyczaj to samo co po prostu .split(), ale nieraz troszkę inaczej np: tak? -> ['tak', '?'] lub ['tak?']
Zazwyczaj tokenizacja to po prostu dzielenie wektora na słowa (split), ale mozna tez na chars, lub sub-words

Stopword - usuniecię słów takich które sie powtarzaja czesto i praktyucznie nei maja zanczenia, wbudowane do englis
do polskiego nie ma ale sa jakies na githubie warto sprobowac z/bez i porownac wyniki (*)

Stemming - run/running -> w to samo, tzw root word żeby uniknąc high dimensionalty, jest dużo różnych algorytmów jeden z popoularnych
to PoperStemmer z nltk.
Stemming - zasady np. utnij ing
Lemmatization - słownik - mapuje coś na coś
W tym filmie tłumaczy jak zastosować Stemming/Lemmatizaiton w praktyce. Jak zbudować obiewkt i odrazu przekazać go do CountVectorizera.
Tzn. jak odrazu liczyć 'walking' i 'walk' jako jedno i wektorze numerycznym.
Robi obiekty tych klas i prosto daje je do countVectorizera który zgodnie ze Stem/Lem zlicza słowa i tworzy już wektory o RÓŻNYCH
 wymiarach w zaleznosci od metody. Po stemmingu/lemm oczywiscie wektory są mniejszych wymiarów, bo kilka słów o praiwe identycznym
znaczneiu jest trakotowane jako jedno. Obiekty wyjsciowe z CountVectorizera (po zastosowaniu stem/lem) prosto do funkcji .fit() modelu.
PRKATYKA w filmie 16.

TF-IDF - 'Ulepszony CountVectorizer' w zasadzie zwraca to samo, tylko zamiast na koncy miec wektor liczb (po prostu policzonych)
ma wektor wag słów - Zamiast zlicxac wystapienia slow ma wagi tych slow. Im slowo czesciej wystepuje w rekordzie i rzadziej w bazie
ma wieksza wage. taki ContVect z uwzglednieneim wystepowania w rekordzie i wystepowaniem w calej bazie. Są róźne wariacje.
Tak samo mozna wrzucic stopwords, tokenizer itd. rozni sie tylko tym ze zamiast zliczonych wystapien, odpowiednie wagi

Word2vec/Glove - narzędzia które zamieniają słowa na wektory numeryczne. To są w zasadzie model (chyba NN) które na wejsciu
dostają zdania, a na wyjsciu zwracają wektory numeryczne dla każdego SŁOWA. Te słowa (wektory numeryczne) są dalej
wykorzystywane w innych modelach np.  NN dla klasyfikacji. Czyli word2vec i glove to modele ktore zdania zamieniaja na wektory numeryczne slow
i te wektory moga byc uzywane w innych modelach juz do konkretnych zadan.
word2vec chyba uzywa NN a glove nie
Tak, w przypadku obu modeli - Word2Vec i GloVe - możemy trenować własny model przez utworzenie nowej instancji modelu i przeprowadzenie treningu.
Możemy również skorzystać z gotowych wag wytrenowanych wcześniej modeli, wczytując plik zawierający mapowanie słów na odpowiadające im wektory numeryczne.
mozna samemu od  zera wytrenowac model na luzie albo wczytac plik (mape) i smigac na tym
jesli nie ma slow to sam autor polecał, tokenizowac tekst i pozniej trenowac open-sourcowe word2vec